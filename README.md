# Glintt_Next_Assessment_Case

End-to-end batch analytics pipeline implemented with **Apache Spark and Delta Lake**, following a **Rawâ€“Silverâ€“Gold data lake architecture**.  
The solution models **Slowly Changing Dimensions (SCD Type 2)** and **fact tables**, and is aligned with an **AWS-oriented architecture** for scalable analytics workloads.

---

## ğŸ“Œ Problem Overview

The goal of this challenge is to design and implement a batch analytics platform capable of:
- Ingesting incremental raw data
- Applying data quality and transformation rules
- Managing historical dimensions
- Producing analytics-ready datasets
- Following enterprise-grade data engineering best practices

The implementation avoids managed SaaS analytics platforms and focuses on open, portable technologies.

---

## ğŸ—ï¸ Architecture Overview

The proposed architecture is based on an **AWS data lake pattern**, implemented locally using Spark and Delta Lake.

ğŸ“ **Raw Zone (Bronze)**  
- Immutable CSV batch files  
- Incremental ingestion  
- Source-of-truth data  

ğŸ“ **Silver Zone (Curated)**  
- Cleaned and conformed datasets  
- Snapshot-level grain  
- Stored as **Delta Lake tables**

ğŸ“ **Gold Zone (Serving)**  
- Analytics-ready dimensional model  
- **SCD Type 2 dimensions**
- Fact tables optimized for BI and SQL analytics

### Architecture Diagram
ğŸ“Œ `diagrams/architecture.png`

---

## â­ Dimensional Model (Star Schema)

The Gold layer follows a **star schema** design:

### Dimensions
- **dim_athlete** (SCD Type 2)
- **dim_games** (SCD Type 0 â€“ immutable reference data)

### Fact Table
- **fact_olympic_results**

ğŸ“Œ `diagrams/star_schema.png`

---

## ğŸ“Š Data Model Details

### ğŸ§ Athlete Dimension (SCD Type 2)
Tracks historical changes in athlete attributes:
- height
- weight

Each change generates a new version with:
- `effective_from`
- `effective_to`
- `is_current`

### ğŸ® Games Dimension (SCD Type 0)
Reference data for Olympic games:
- year
- season
- city

No historical changes are expected.

### ğŸ§® Results Fact Table
Stores event-level results and references both dimensions using surrogate keys.

---

## ğŸ”„ Pipeline Flow

1. **Raw â†’ Silver**
   - Read latest CSV batch
   - Apply schema and data types
   - Deduplicate records
   - Write Delta snapshot

2. **Silver â†’ Gold (Dimensions)**
   - Apply SCD logic using Delta MERGE
   - Preserve full history
   - Enforce one current record per business key

3. **Silver â†’ Gold (Fact)**
   - Resolve surrogate keys
   - Insert new fact records
   - Ensure referential integrity

---

## ğŸ› ï¸ Technology Stack

- **Apache Spark 3.5**
- **Delta Lake**
- **PySpark**
- **SQL (MERGE-based SCD logic)**
- **Mypy** for static type checking
- **AWS-oriented architecture design**

---

## ğŸ“‚ Repository Structure
Glintt_Next_Assessment_Case/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_samples/
â”‚       â”œâ”€â”€ athletes_22-02-2026_10_08.csv
â”‚       â”œâ”€â”€ athletes_22-02-2026_11_02.csv
â”‚       â”œâ”€â”€ athletes_23-02-2026_16_10.csv
â”‚       â”œâ”€â”€ games_23-02-2026_10_24.csv
â”‚       â”œâ”€â”€ games_23-02-2026_10_31.csv
â”‚       â”œâ”€â”€ results_fact_batch_23-02-2026_11_41.csv
â”‚       â””â”€â”€ results_fact_batch_23-02-2026_11_50.csv
â”‚
â”œâ”€â”€ diagrams/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ star_schema.png
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ pipeline_athlete.py
â”‚   â”œâ”€â”€ pipeline_games.py
â”‚   â””â”€â”€ pipeline_fact.py
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ mypy.ini
